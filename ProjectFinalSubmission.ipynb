{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dee85ba",
   "metadata": {},
   "source": [
    "# Project Context - Data Science Challenge for an Interview\n",
    "\n",
    "This project is a data science challenge I completed for a small, boutique consulting firm. I liked this project and wanted to share because it requried me to learn something new. I had to identify the correct approach to solve this problem, find a library that implements the appropriate approach, and make slight adjustments to my model evaluation (standard sci-kit learn libraries would not support the model evaluation methods).\n",
    "\n",
    "This problem required me to learn about Positive-Unlabeled learning (PU Learning). Instead of the traditional Case-Control or 1-0 format, this data takes on the form of Positive and Unlabeled, as in, we don't know if the unlabeled observations would be positive or negative because we haven't applied a treatment. We only know that the positives are positives.\n",
    "\n",
    "As a disclaimer upfront, this interview process did not have an NDA. I have completed other interview processes that did have an NDA and you will not find them in my gitHub account, or anywhere else.\n",
    "\n",
    "# The Text of the assignment\n",
    "\n",
    "\n",
    "### Challenge Overview:\n",
    "\n",
    "A client has approached us with a dataset of their clients, including various attributes like interactions with services, demographic details, and financial information. Each client is labeled either as \"Advisory\" (those who are using advisory services) or\n",
    " \"Unknown.\" The \"Unknown\" label represents clients for whom it is unclear whether they don't need advisory services or might be great candidates for it but haven't opted for the service yet.\n",
    "\n",
    "In addition to the original dataset, the client has provided a secondary dataset of client transactions that includes dates, amounts, and types of transactions. They suspect there may be a relationship between a client’s most recent transaction, the most frequent\n",
    " type of transaction, the total amount of transactions, and the likelihood of needing advisory services.\n",
    "\n",
    "### Your Task:\n",
    "\n",
    "Your challenge is to build a model that predicts which of the \"Unknown\" labeled clients are potential candidates for advisory services. This involves analyzing both datasets to identify patterns or characteristics—including financial behaviors indicated by\n",
    " the transaction data—that distinguish potential candidates for advisory services.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0219abb",
   "metadata": {},
   "source": [
    "# My work begins here: \n",
    "\n",
    "## Pre-analysis reading and understanding\n",
    "\n",
    "I started by reading the prompt a few times and taking notes on the key pieces of information. There are two datasets and the target variable contains *Positives* and *Unknowns.* Typically, there are *Positives,* *Negatives,* and *Unknowns.* This is new for me, so I went to every Data Scientist's favorite tool - Google.\n",
    "\n",
    "\n",
    "#### Elkan and Noto\n",
    "My searches lead me to the field of PU Learning. While the name leaves something to be desired, I found a great paper by [Elkan and Noto](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://cseweb.ucsd.edu/~elkan/posonly.pdf&ved=2ahUKEwi2pYizvNmFAxXPkIkEHa67B8UQFnoECA8QAQ&usg=AOvVaw1o-E1rzGkbTquUxZtb69md) on the subject. The paper is great and the proof is easy to follow (I only followed it loosely, there are enough citations on Google that I trust it's validity). The paper makes an adjustment to our traditional machine learning process that enables PU Learning under the assumption that the positives are SCAR (Selected Completely At Random). This assumption may not hold 100%, but I think the process of getting clients is sufficiently random that this should hold\n",
    "\n",
    "#### The Naive Approach\n",
    "The Naive approach assumes that \"Unknown\" and \"Negative\" are the same. I don't like this at all because that assumption doesn't hold. The name seems accurate here and I find this approach unpalatable.\n",
    "\n",
    "#### imPULSE\n",
    "\n",
    "I found a towards data science article that proposes another method that is relatively complex, but performs well on imbalanced data. There is no mathematical proof and I didn't see any peer reviews on this (they may exist, I didn't spend a ton of time searching for them). This makes me nervous to implement the method. I tend to gravitate towards research that is published and peer reviewed because I trust the rigorous review process. That process is capable of missing errors, but it does so infrequently.\n",
    "\n",
    "#### Selected Method: Elkan and Noto\n",
    "Peer reviewed and a simple but elegant proof? Sign me up. The assumption might not hold 100%, but I'm more comfortable with that as opposed to the other two methods.\n",
    "\n",
    "## My Plan:\n",
    "\n",
    "Every good project begins with a plan to guide it.\n",
    "\n",
    "1) Inspect the Advisory (primary) dataset\n",
    "2) Clean the Advisory dataset, noting any questionable decisions and assumptions\n",
    "    - Be ready to share those assumptions and cleaning steps with clients to build trust and gather their feedback. Transparency helps data scientists become trusted advisors by eliminating the smoke and mirrors.\n",
    "3) Visualize the data and generate some hunches about features. This may help with feature selection (if necessary)\n",
    "4) Inspect the Transaction data\n",
    "5) Build the features\n",
    "    - I considered engineering additional features. I love feature engineering, but this is a rush job. I decided to avoid feature engineering in the interest of time  \n",
    "6) Join the data prior to analysis, confirm there are no NAs or other flaws\n",
    "7) Fit a model\n",
    "8) Parameter tune via CV-folds\n",
    "    - For this, I will be deviding the data into 10-folds. Each fold will have 2 'Advisory' observations that have been artificially flipped to 'Unknown'. The model's performance will be it's ability to correctly label those values as Advisory.\n",
    "9) Take the best model, generate labels.\n",
    "10) Prep for Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeaa265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from pulearn import ElkanotoPuClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "### import my functions\n",
    "from cleaningAndFeatures import *\n",
    "\n",
    "seedA = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242375c3",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = pd.read_csv(\"advisory.csv\")\n",
    "trans = pd.read_csv(\"client_transactions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d545f1e",
   "metadata": {},
   "source": [
    "## 1 -  Inspect the Advisory data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5267e0eb",
   "metadata": {},
   "source": [
    "As expected, there is an ID and Label for each observation. Notably, there are 20 NAs for Age and Income - we will need to inspect these to handle them correctly. Additionally, we will want to inspect the factor levels for anything erroneous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the shapes match, therefore we have complete cases for all advisory data. Excellent.\n",
    "print(adv[adv['Label'] == 'Advisory'].shape)\n",
    "print(adv[adv['Label'] == 'Advisory'].dropna(axis = 0, how = 'any').shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8121538",
   "metadata": {},
   "source": [
    "### Handling NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48f72d",
   "metadata": {},
   "source": [
    "The 20 NAs are present for Age and Income. This could be used as a justification to drop these observations from the dataset; however, the case stated the clients *\"suspect there may be a relationship between a client’s most recent transaction, the most frequent type of transaction, the total amount of transactions, and the likelihood of needing advisory services.\"* None of those features have anything to do with Age or Income, so dropping them could eliminate a strong potential candidate. \n",
    "\n",
    "I'm going to add the mean by factor-level when cleaning this data. There should be an average value of age and income for each combination of Job Type and Tax Classification. This should be sufficient in this case. I'm going to write a quick function that I will call during the main cleaning function at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d890eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv[adv['Age'].isna()].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8af0c5",
   "metadata": {},
   "source": [
    "### Inspect the Factor-level data\n",
    "\n",
    "Next, we want to identify any erroneous factor-level data in Job Type and Tax Classification.\n",
    "\n",
    "#### Job Type\n",
    "It seems clear that 'Vr' is intended to be 'VP' - I am going to make this change. Chief Officier appears to be a misspelling, but it is uniform and won't impact the analysis. I'm going to leave this alone.\n",
    "\n",
    "#### Tax Classification\n",
    "There are a number of examples where the final character of the string was changed to 'd' instead of the correct character. I am going to change these in the cleaning script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv['Job Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv['Tax Classification'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f65e1",
   "metadata": {},
   "source": [
    "### Analysing the Continuous Variables\n",
    "\n",
    "To analyze these variables, we want to look at the descriptive statistics as well as a plot of the distribution of values. A histogram or KDE would work here, but I prefer a KDE because it gives me a better idea of the shape. The function below will create the KDE and a few descriptive stats.\n",
    "\n",
    "There are no erroneous values in the continuous variables. There are some values that have a small decimal value, but that won't be material for modeling a continuous variable. Still, we will round these during cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56876203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptiveKDE(df, variable):\n",
    "    vals = df[variable][df[variable].notnull()].values\n",
    "    posVals = df[variable][df['Label'] == 'Advisory']\n",
    "    unlabVals = df[variable][(df['Label'] != 'Advisory') & df[variable].notnull()]\n",
    "    fig = sns.kdeplot(vals, color = 'black', label = 'All Values')\n",
    "    fig = sns.kdeplot(posVals, color = 'blue', alpha = .7, label = 'Positives')\n",
    "    fig = sns.kdeplot(unlabVals, color = 'red', alpha = .7, label = 'Unlabeled')\n",
    "    \n",
    "    med = round(np.median(vals))\n",
    "    mean = round(np.mean(vals))\n",
    "    min = round(np.min(vals))\n",
    "    max = round(np.max(vals))\n",
    "    text = \"Min: {} | Max: {} | Median: {} | Mean: {}\".format(min,max,med,mean)\n",
    "    \n",
    "    plt.axvline(x = mean, color = 'green', label = 'Mean', linestyle = '--')\n",
    "    plt.axvline(x = mean, color = 'purple', label = 'Median', linestyle = '--')\n",
    "    \n",
    "    return fig, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd86b7db",
   "metadata": {},
   "source": [
    "#### Income Density and Descriptive Stats\n",
    "\n",
    "The mean and median are quite close, which explains why the mean is not visible on the plot. They are also near the maximum density on the plot, which suggests either would approximate an MLE in this case.\n",
    "\n",
    "Notably, the Positive values have a narrower distribution compared to the Unlabeled values. The maximum Positive value is less than 1M, where as there are many Unlabeled observations greater than 1M. As such, it appears that this variable may help identify some negative values within the Unlabeled group. This is something we will want to test when modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xticks = [0, 1000000, 2000000, 3000000, 4000000]\n",
    "xlabels = ['0', '1,000,000', '2,000,000', '3,000,000', '4,000,000']\n",
    "fig, text = descriptiveKDE(adv, 'Income')\n",
    "plt.title('Income Density Plot')\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.xticks(xticks, xlabels)\n",
    "plt.legend()\n",
    "print(text)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b47e082",
   "metadata": {},
   "source": [
    "As shown in Income, the mean and median are nearly the same. This distribution appears to be bimodal. There are maximums near 28 and 50. Unlike Income, this variable does not show any visible potential to separate the unlabeled data; however, there may be an effect that isn't visible. This will still be considered in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fafcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, text = descriptiveKDE(adv, 'Age')\n",
    "plt.title('Age Density Plot')\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.legend()\n",
    "print(text)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e87054a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec348142",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(adv['Income'].loc[adv['Income'].notnull()],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97aad8",
   "metadata": {},
   "source": [
    "## 2 - Clean the Data\n",
    "\n",
    "This happens explicitly below, but it has also occurred in the steps above via the functions I wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28beaa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c65886",
   "metadata": {},
   "outputs": [],
   "source": [
    "advClean = cleanAdv(adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007803f9",
   "metadata": {},
   "source": [
    "## 3) Visualize a few interactions\n",
    "\n",
    "We've already done some EDA on Age and Income, forming the hypotheses that Income may help identify potential candidates and that Age appears unlikely to be helpful. Now we will explore Tax Classification and Job Type via mosaic plots. This will treat Label and either Tax Classification or Job Type as a pair of factors, forming a two-way table and applying some visualization to it. This should provide some insight into a potential association between Label and the two factors.\n",
    "\n",
    "We could go one-step further and perform a chi-sq test for independence, but that is unnecessary based on the text in the challenge. In a client setting, I may choose to do this if I felt the client would be interested in the extra knowledge or seeing the analysis.\n",
    "\n",
    "#### Tax Classification\n",
    "\n",
    "The unknown labels appear to be divided into perfect thirds, whereas the Advisory clients are a majority \"Medium\" for Tax Classification. Any model fit will likely favor the \"Medium\" category, but there are clearly advisory clients in the other two categories. Hopefully there is a combination of variables or interaction in the data that will help separate these. Regardless, this variable appears to be useful for identifying good candidates\n",
    "\n",
    "#### Job Type\n",
    "\n",
    "The distinction between the advisory and unknown labels is less pronounced for Job Type. This suggests that this variable may not be as useful for identifying good candidates within the unknown label. Additionally, there appears to be close to an even split among the 3 levels of Job Type and Advisory clients. The Chief Officier label appears to be more present than others - maybe this has a slight effect, but I have some doubts.\n",
    "\n",
    "##### Overall Hypothesis\n",
    "I strongly suspect that Tax Classification will be useful for identifying potential candidates and that Job Type is less useful. Nevertheless we will test both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02992078",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = mosaic(advClean, ['Label', 'Job Type'])\n",
    "fig = plt.title(\"Label vs Job Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324dfeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = mosaic(advClean, ['Label', 'Tax Classification'])\n",
    "fig = plt.title(\"Label vs Tax Classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ffa19",
   "metadata": {},
   "source": [
    "## 4 - Generating Features with the Second Dataset\n",
    "\n",
    "The basic exploration and cleaning within the primary dataset is complete, so we will move onto the second dataset. The challenge suggests 3 features may be useful based on the client's suspicions\n",
    "\n",
    "The features are: \n",
    "\n",
    "1) Most Recent Transaction Data\n",
    "2) Most Frequent Type of Transaction\n",
    "3) Total Number of Transactions\n",
    "4) Total Amount of Transactions (Dollars)\n",
    "\n",
    "My suspicion is that 2 and 3 are likely useful; however, 1 feels like it'd be more random than helpful. Regardless, we will develop, test, and analyze all 3, communicating our findings to the client. It's odd that there isn't a feature for total transaction amount or net transaction amount. My guess would be to start with total transaction amount because I would guess there is a connection between the dollar amounts transacted and tax complexity (as in, people doing more business likely have more complex tax situations). This may already be represented in Tax Classification, but it won't hurt to try. This feature is easy to make, so I will make it.\n",
    "\n",
    "*Note: I would explore additional features as part of the challenge and to meet client needs. However, I am skipping this in the interest of time as this is a rush job to meet the timeline. I am of the belief that feature engineering and exploration, particularly interaction effects, are worth exploring to improve models.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d752e7a1",
   "metadata": {},
   "source": [
    "## Inspect the transaction data\n",
    "\n",
    "We have no nulls and the types are as expected. Clearly the transaction date needs to be changed to a datetime via pd.to_datetime. This is standard as pandas does not automatically recognize dates. It is also a reason why I advocate for pkl files whenever possible.\n",
    "\n",
    "### IDs\n",
    "I checked to verify that all of the IDs are present, and they are not. Quick list comprehension shows that 3 Advisory and 17 Unknown observations are not included in this dataset. I'd imagine this is due to 1) client data quality, which is notoriously poor, or 2) there was no transaction activity for the given date range.\n",
    "\n",
    "In practice, I would ask the client about 1). It's always possible someone made a small error that caused the observations to be omitted from the data. Due to time constraints, I am going to assume there was no activity and input logical labels such as \"None\" or 0 based on the context.\n",
    "\n",
    "### Transaction Type\n",
    "\n",
    "I didn't discover anything odd here. There are three categories and a \"None\" category will be added for the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f36e26",
   "metadata": {},
   "source": [
    "#### IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b0fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "allIDs = list(range(1,201))\n",
    "transIDs = trans['ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfcf2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "missingIds = [x  for x  in allIDs if allIDs.index(x) not in transIDs] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ac265",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take a peak at the missing IDs and see if there is a clear trend or pattern\n",
    "## None are present. i was hoping for something like all 20 missing IDs are missing\n",
    "## Age and Income - I would drop those IDs, instead I have to be creative.\n",
    "adv[adv['ID'].isin(missingIds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74486d8e",
   "metadata": {},
   "source": [
    "### Transaction Type\n",
    "\n",
    "There are three types of transactions and no misspellings. No cleaning required here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans['Transaction_Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f3529",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans['Transaction_Date'] = pd.to_datetime(trans['Transaction_Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6599bfe",
   "metadata": {},
   "source": [
    "### Amount\n",
    "\n",
    "We know none of these are null and we aren't using the individual transactions, rather the sum of transaction amounts. We will perform some light EDA on this feature after cleaning and prepping the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686e2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab1387fc",
   "metadata": {},
   "source": [
    "### Transaction Date\n",
    "\n",
    "For this, I am going to identify the minimum and maximum transaction dates to understand the range, then use a date well in advance of the minimum as the NA filler for any missing values. Logically, this has to check out because they have not transacted in the range, and time is linear. Additionally, the client believes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff3448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cleanPrepFeatures(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ea6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89335f",
   "metadata": {},
   "source": [
    "## Pre-Modeling, Merge all Data\n",
    "\n",
    "We now have a complete dataset that is ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4dc910",
   "metadata": {},
   "outputs": [],
   "source": [
    "## join it\n",
    "modelingData = advClean.merge(features, how = 'left', on = 'ID')\n",
    "\n",
    "## SKLearn does not like text in columns, drop_first to prevent overparameterizing a model\n",
    "#X = pd.get_dummies(modelingData.drop([\"ID\", 'Label'], axis = 1))\n",
    "X = modelingData.drop([\"ID\", 'Label'], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "## get the y vector (s in the paper)\n",
    "y = np.where(modelingData['Label'] == 'Advisory',\n",
    "             1,\n",
    "             0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc96974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelingData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea50d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ab9dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initiallize an encoder for each type - we can transform back easily if needed\n",
    "leJT = LabelEncoder()\n",
    "leTC = LabelEncoder()\n",
    "leMRT = LabelEncoder()\n",
    "leMFT = LabelEncoder()\n",
    "\n",
    "X['Job Type'] = leJT.fit_transform(X['Job Type'])\n",
    "X['Tax Classification'] = leTC.fit_transform(X['Tax Classification'])\n",
    "X['mostRecentType'] = leTC.fit_transform(X['mostRecentType'])\n",
    "X['mostFrequentTrans'] = leTC.fit_transform(X['mostFrequentTrans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d136731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f99f8",
   "metadata": {},
   "source": [
    "### Fit a model\n",
    "\n",
    "I chose a Random Forest because I don't have to perform variable selection and this is a rush job. Cutting out a step helps. In general, I would try mutliple different SKLearn estimators here to be thorough.\n",
    "\n",
    "Step 1 is to get a model fit and make some predictions. We don't care about the predictions because this is not cross-validated, but I will check them. The next part will be some hyperparameter tuning, and then we will fit and recover the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc14f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svc = SVC(C=10, kernel='rbf', gamma=0.4, probability=True)\n",
    "rfClass = RandomForestClassifier(random_state=seedA)\n",
    "EN = ElkanotoPuClassifier(estimator=rfClass, hold_out_ratio=0.2)\n",
    "\n",
    "### Typically SKLearn can accept a dataframe for X, however, this is throwing an error\n",
    "### It's probably the Elkan Noto implementation, I'm not going to crack this open right now\n",
    "EN.fit(X.values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yHat = EN.predict(X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb5cd2",
   "metadata": {},
   "source": [
    "The model that without tuning thinks there are 65 potential clients in this dataset. Cool! We probably want to explore these.\n",
    "\n",
    "The context of this problem is a bit different than standard ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28123e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(yHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a9f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelingData['yHatEN'] = yHat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ae949",
   "metadata": {},
   "source": [
    "The model predicted 100% of the current candidates are potential candidates. That's very good, but we may want to play with the threshold to prevent bad predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3529c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(modelingData['yHatEN'][modelingData['Label'] == 'Advisory'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e3d93",
   "metadata": {},
   "source": [
    "### Cross-validated grid search\n",
    "\n",
    "Let's tune the random forest. I won't constrict the features because there are a ton of factors in there, and only including some factor levels feels odd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6fb159",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfParams = {'n_estimators': [100,300,500], ## try a few arbitrary numbers of trees\n",
    "          'max_depth': [2, 5, 10, None], ## set the maximum number of partitions\n",
    "          'min_samples_split': [2, 10, 30], ## try constraining the num obs at the nodes\n",
    "          'max_features': [None] ## there are 3 predictors, constraining the features is useless\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5432a1",
   "metadata": {},
   "source": [
    "One of the fun things about using GridSearchCV here is that I have to build the estimators and pass them through. It will not accept the rf params. I'm going to write a for loop that builds classifiers and adds them to a list, which will go into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd43481",
   "metadata": {},
   "outputs": [],
   "source": [
    "allModels = []\n",
    "\n",
    "for est in rfParams['n_estimators']:\n",
    "    for depth in rfParams['max_depth']:\n",
    "        for feats in rfParams['min_samples_split']:\n",
    "            rfClass = RandomForestClassifier(n_estimators = est,\n",
    "                                             max_depth = depth,\n",
    "                                             min_samples_split = feats)\n",
    "            allModels.append(rfClass)\n",
    "ENParams = {'estimator': allModels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b19aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfClass = RandomForestClassifier()\n",
    "EN = ElkanotoPuClassifier(estimator = rfClass, hold_out_ratio=0.2)\n",
    "\n",
    "grid = GridSearchCV(EN, ENParams, cv = 10, scoring='recall')\n",
    "grid.fit(X.values,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157aa0e",
   "metadata": {},
   "source": [
    "This is so bad that I suspect the F1 Score calculation in sklearn might not work properly for PU data, which would require me to code something custom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09918664",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff893574",
   "metadata": {},
   "source": [
    "## Findings\n",
    "The F1 score qualifies as abysmal.y suspicion is that the baked in methods for GridSearchCV don't work properly because of the PU data as opposed to the traditional case-control format for classification. Seeing an F1 of .25 after 95% correct labels above doesn't make sense to me. I probably need to do additional research to determine the best way to evaluate a PU model's performance.\n",
    "\n",
    "### Next Steps I would take\n",
    "I would go back to the drawing board and try an SVM here to see if that improved the performance. If the performance is still poor, I would draw up a custom hyperparameter tuning approach as that could indicate that my suspicions are correct. In this, I would make 10 CV folds, holding out exactly 2 Positives and 18 Unknowns. Train on the remaining 180 observations and test the remaining 20, paying close attention to the predicted classes for the Positive labels. If my model identified the positive labels well, then I would assume it is identify all Positive labels well. I would test this for the random forest and SVM approaches, as it's not too much more of a lift.\n",
    "\n",
    "### Client Communication\n",
    "\n",
    "I would tell the clients all of the work we did to clean and prep, as well as the research we performed to ensure we are applying the correct models. Then I would talk about all of the models we trained. The purpose of this would be to communicate that we worked hard, paid close attention to detail, and they are receiving value for their money. Then I would go over the model performance, finally delivering the labeled values to the best of our ability.\n",
    "\n",
    "I would request feedback after this is applied. Did the model perform well? Is there something we could adjust to make this better? Did the end user understand the potential candidates we supplied?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d634d",
   "metadata": {},
   "source": [
    "## Final Submission\n",
    "\n",
    "I'm going to make a final submission even if the modeling exercise is light by my standards. I'm going to use the basic RF classifier that correctly identified 80% of the positives, as the parameter tuning was unsuccessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8061a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = modelingData[['ID','yHatEN']].rename(columns= {'yHatEN':'predicted_label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801345ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51979bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
